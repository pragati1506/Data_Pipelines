# Data_Pipelines

    ##What is Apache Airflow?

  Apache Airflow is a workflow automation platform that is popular for its open-source availability and scheduling capabilities.
You can utilize this tool to programmatically author, schedule, and monitor any number of workflows.
Businesses today use Airflow to organize complex computational workflows, build data processing pipelines, and easily perform ETL processes.
Airflow operates on DAG (Directed Acyclic Graph) to construct and represent its workflow, and each DAG is formed of nodes and connectors.
These Nodes depend on Connectors to link up with the other nodes and generate a dependency tree that manages your work efficiently.


    ##What is a Data Pipeline?

  A Data Pipeline consists of a sequence of actions that can ingest raw data from multiple sources,
transform them and load them to a storage destination.A Data Pipeline may also provide you with end-to-end management 
with features that can fight against errors and bottlenecks.



Extract, transform, and load (ETL) orchestration is a common mechanism 
for building big data pipelines. Orchestration for parallel ETL processing requires 
the use of multiple tools to perform a variety of operations. Data orchestration is the 
process of taking siloed data from multiple data storage locations, combining and 
organizing it, and making it available for data analysis tools. Data orchestration 
enables businesses to automate and streamline data-driven decision-making
